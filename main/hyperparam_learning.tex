\chapter{Hyperparameter distribution learning}
	\label{ch:learn}
	Applying a SML algorithm on different sets of data usually requires that the hyperparameters
	adopt different values to achieve the best performance. However, there are certain ranges of values
	that are harmful in all but some pathological cases, and likewise, some regions of the
	hyperparameter space produce configurations that consistently perform well when applied on
	different types of data, and hence are worth trying when searching for good models.

	This suggest that it might be helpful to guide the hyperparameter optimization by making use of
	some description of the hyperparameter space that gives hints about what values a given
	hyperparameter should and should not take. Starting the sampling of hyperparameter values from a
	hyperparameter-specific {\bf general prior distribution} that has high density around regions of
	the hyperparameter space known to perform well, and low or no density around regions known to
	harm the prediction of the SML algorithm, will help the optimization process to discover more
	quickly where the local optima might be, and what regions of the hyperparameter space it is safe
	to avoid.
	
	The implemented approach evaluates a large number of configurations on a group of datasets with
	different properties, and uses the performance indices obtained to learn the general prior
	distribution for each numerical hyperparameter individually. Using a large number of datasets
	reduces the risk of overfitting the general prior learned to particularities in the evaluation
	of a single dataset.

	\subsubsection{Parametric vs non-parametric modeling of the general prior distribution}

	Due to the large number of configurations and dimensions to test, a compact representation of
	the general prior distributions is preferred. Estimation of parametric distributions is
	convenient because it allows for this compactness and because usually closed-form solutions for
	maximum likelihood fitting of their parameters exist. 	

	Non-parametric techniques to estimate the general prior distribution, like Kernel Density Estimation
	\cite{rosenblatt1956kde}, \cite{parzen1962kde} keep too much information in memory, are cumbersome to
	encode and store for later use, and depend on heuristics and rules of thumb to decide parameter
	values such as the kernel bandwidth, and are therefore not suitable for the automatic approach
	presented in this work.

	\section{Learning the general prior distribution}
	\label{sec:learning_prior}

	The chosen representation for the general prior distribution is a Gaussian Mixture Model, which
	is convenient for encoding multimodal distributions and to which it is not very expensive to fit
	data. Since the true probability of a hyperparameter value being the optimal is not known, the
	performance index of models containing such value is used as a surrogate for the estimation of
	this probability, as it measures the quality of the prediction. 
	The implementation of the learning process is a Montecarlo method that accepts or rejects
	values of the hyperparameters according to their performance indices, summarized in algorithm
	\ref{alg:general_prior}.

	\begin{algorithm}[here]
		\begin{algorithmic}
			\Function{learn\_general\_prior}{training\_datasets, threshold}
				\State general\_prior $\gets$ initial GMM
				\For {dataset $D$ in training\_datasets}
					\While {convergence criterion not met}
						\State Sample a value $\lambda$ for the hyperparameter from a uniform
						distribution
						\State score $\gets$ performance\_index($\lambda, D$)
						\If {random$(0,1) \leq$ score}
							\State \Call{combine}{general\_prior, $\lambda$}
							\State \Call{simplify}{general\_prior, threshold}
						\EndIf
					\EndWhile
				\EndFor
				\State
				\Return {general\_prior}
			\EndFunction
		\end{algorithmic}
		\caption{General prior distribution learning}
		\label{alg:general_prior}
	\end{algorithm}

	The algorithm evaluates at each iteration if a more complex GMMs (more Gaussian components) fits
	the data significantly better than the current GMM.  In order to find a trade-off between the
	GMM complexity and predictive power, the proposed idea is to measure the \emph{fidelity} of the
	mixture model (i.e.~a quantitative estimate of its ability to describe the associated data, as
	described in \cite{declercq2007online, declercq2008online} for the \emph{uncertain Gaussian
	model}), and to define a fidelity threshold below which a mixture model is considered overly
	simplistic. Mixture models will be updated upon arrival of new observations and simplified if
	the above condition holds.

	\subsubsection{Learned model update}
	Everytime a hyperparameter value is sampled and evaluated, its evaluation is added as a new
	component to the mixture model, by trivially combining the existing distributions with the new
	one.

	If at any given point in time $t$ the mixture model is expressed as a set of $N$ weighted
	Gaussian distributions:
	\begin{equation}
		p^{(t)}(x) =
			\frac
			{\sum_{i=1}^N \pi_i^{(t)}\,g(x; \mu_i^{(t)}, \sigma_i^{(t)})}
			{\sum_{i=1}^N \pi_i^{(t)}}
	\end{equation}

	Then the new mixture model for $t+1$ is given by:
	\begin{equation}
		\label{eq:merge_gaussians}
		p^{(t+1)}(x) =
		\frac
		{\left( \sum_{i=1}^N \pi_i^{(t)} \, g(x; \mu_i^{(t)}, \sigma_i^{(t)}) \right) +
		\pi^{(t+1)} \, g(x; \mu^{(t+1)}, \sigma^{(t+1)})}
		{\left(\sum_{i=1}^N \pi_i^{(t)} \right) + \pi^{(t+1)}}
	\end{equation}

	\subsubsection{Learned model simplification}
	Adding new components to the GMM everytime a new observation arrives hinders all the advantages
	of using parametric estimation. If possible, merging similar components of the GMM into one
	should be done, in order to simplify the mixture model.

	As proposed in \cite{declercq2008online}, two Gaussian distributions (two components of the GMM in
	this case) can be merged without significant loss of predictive power if the merged distribution
	has a \emph{fidelity} $\lambda$ close to 1, otherwise, both components must be kept to
	properly describe the distribution of the data.

	The distance between the distribution to test, and the data to which it should fit, is given by:
	\begin{equation}
		D = \frac 1 {|I|} \int_I \left| \hat{F}(x) - F_n(x)\right| dx
	\end{equation}

	Declercq and Piater assume such distances as Gaussian distributed, and define the
	\emph{fidelity} $\lambda$ as:
	\begin{equation}
		\lambda = e^{\frac{-D^2}{T_D^2}}
	\end{equation}

	With $T_D^2$ a parameter that controls how much deviation from the observations is allowed and
	hence how relaxed the computation is about considering the merged Gaussian as an acceptable
	representation of the two merged components.

	Two Gaussian components $G_i$ and $G_j$ can be merged into one by applying the following
	procedure:

	\begin{align}
		\pi = & \pi_i + \pi_j\\
		\mu = & \frac 1 \pi \left( \pi_i \mu_i + \pi_j \mu_j\right)\\
		\sigma = & \frac {\pi_i} {\pi} \left( \sigma_i + (\mu_i - \mu)^2 \right) + 
			      \frac {\pi_j} \pi \left( \sigma_j + (\mu_j - \mu)^2 \right) \\
				  G(x) = & \mathcal{N}(\mu, \sigma)
				  \label{eq:simplify_gaussians}
	\end{align}
	
	\section{Using the general prior for hyperparameter optimization}

	The distribution returned by algorithm \ref{alg:general_prior} is a rough description of the
	quality of the hyperparameter in different regions of the hyperparameter space. It can be used
	as a prior distribution for sampling candidate hyperparameter values to evaluate on a
	specific dataset.

	Since the general prior contains hints of where, in general, high and low-scoring regions of the
	hyperparameter space are, but is not guaranteed to fit the true distribution of scores for the
	specific data set, further exploration of other regions of the hyperparameter space not
	favored by the general prior might be necessary.
	
	The ideal case would be that the general prior already fits the true underlying distribution of
	scores for the specific dataset, in which case it would suffice to sample values from the
	general prior, and assume that high-scoring values will be retrieved often. As this is seldom
	the case, the implementation continuously assesses the goodness of fit of the evaluations to the
	general prior, and starts to draw samples from an alternative uniform distribution when the
	fitting to the general prior drops below a threshold.


