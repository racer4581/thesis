\chapter{Problem description}

	This work aims to propose a solution for a two-fold problem: find the SML algorithm that works
	best for a given set of data, and find which particular choice of configuration settings for
	that algorithm, if required, yields the best results.
	
	Although both sides of the problem correspond conceptually to different ideas (general approach
	the former, fine-tunning the latter), they are intrinsically linked and should not be treated as
	separate problems. This is especially true for the present work, since a poor choice of
	settings for an otherwise well-suited SML algorithm may be easily outperformed by another, less
	appropriated SML method, applied on the same data.

	The formal description of the problem presented here is given in terms of the classification
	problem. This description extends naturally to the regression problem, but the specific
	terminology for regression has been excluded for the sake of brevity.

	\section{Formal definition}

	The following terms and notation will be used for the rest of this document:

	\begin{itemize}
		\item
		An {\bf instance} is a representation of an object, encoded as a pair $(\feature, \class)$,
		with $\feature$ a vector of values for attributes (also called \emph{features}) related to
		the object, and $\class$ a mapping of the object to its corresponding class.
		
		\item
		A {\bf dataset} $\alldata = \{(\feature_1, \class_1), (\feature_2, \class_2), \ldots,
		(\feature_N, \class_N)\}$ is a set of instances on which SML is to be applied.
		%$\alldata$ is split into a {\bf selection~set} $\selectionset$ and a {\bf testing~set}
		%$\testset$, such that
		%\begin{equation}
		%	\selectionset \cup \testset = \alldata \\
		%	\selectionset \cap \testset = \emptyset \\
		%	\selectionset \neq \emptyset
		%\end{equation}

		\item
		An {\bf algorithm} $\algo$ is a specific implementation of any function that uses known
		instances~$(\feature, \class)$ (also known as \emph{examples}) to learn patterns or rules to
		associate the feature values $\feature$ to their class $\class$, and uses this
		information to predict the class $\class'$ of unseen instances from their feature values
		$\feature'$.
		\begin{equation}
			\label{eq:sml_alg}
			A \colon (\{(\feature, \class)\}, \{\feature'\}) \mapsto \{\class'\}
		\end{equation}

		\item
		$\algoset = \{\algo^1, \ldots, \algo^k\}$ represents the set of all algorithms to evaluate.
		
		\item
		An algorithm $\algo$ accepts a set of {\bf hyperparameters} $\paramset_\algo$ that modify
		its behavior.

		\item
		$\paramval_a$ is the value of a {\bf hyperparameter} $a \in \paramset_\algo$ for some
		algorithm $\algo$.

		\item
		$\param^i = (\paramval^i_a, \paramval^i_b, \paramval^i_c,\ldots)$ denotes a set of
		hyperparameter values (or {\bf configuration}) for the $i$-th algorithm in $\algoset$.
		
		\item
		$\paramspace^i$ represents the set of all possible values that $\param^i$ can take (i.e. the
		{\bf hyperparameter~space} of the $i$-th algorithm).

		\item
		An algorithm $\algo$ under a specific configuration $\param$ is called a {\bf model}, and
		represented as $\algo_{\param}$.

		\item
		The {\bf scoring} function $\Score$ measures how good a model $\algo_{\param}$ is at
		predicting the class of unseen instances drawn from the dataset
		$\alldata$.
		\begin{equation}
			\Score \colon \algo^i \in \algoset, \param^i \in \paramspace^i, \alldata \mapsto
			\mathbbm{R}
		\end{equation}
		The scoring function should be designed in such a way that the better the agreement between
		class prediction and true class assignment is, the higher the score.
	\end{itemize}

	It is assumed that the feature values of the examples $\feature$ used for training an algorithm
	and the feature values to predict labeling $\feature'$ are drawn from the same underlying
	distribution, and that instances with similar feature values tend to belong to the same class.

	Under the described context, the model selection and hyperparameter optimization problem can be
	defined as:
	\begin{equation}
		\label{eq:problem}
		\algo^*_{\param^*} = \argmax_{\algo^i \in \algoset, \param^i \in \paramspace^i}
		\Score(\algo^i_{\param^i}, \alldata)
	\end{equation}

	The above equation can be simply stated as "find the algorithm and its parameter values that
	obtain the best score at predicting labels on the given dataset". It is worth noticing that
	equation~\ref{eq:problem} is the general form of the optimization process, and as such, only
	defines the structure and general behavior of the different components of the optimization. In
	practice, additional details such as the implementation of the scoring function, and the actual
	exploration of the (possibly infinite) hyperparameter spaces, must be considered.

	Furthermore, the assumption that a single model $\algo^*_{\param^*}$ will be \emph{significantly} better than the
	rest is not guaranteed, and hence returning multiple models as the result of the
	optimization process should also be considered under certain circumstances.

	\section{Related work}

	Few initiatives to address the model selection and hyperparameter optimization of SML algorithms
	have been proposed. 

	For hyperparameter optimization, the \emph{de facto} method, known as \emph{grid search}, is the
	exhaustive evaluation of a discretization of the hyperparameter space on a regularly-spaced grid. Such method is
	affected by the \emph{curse of dimensionality} when a large number of hyperparameters needs to
	be analyzed, and the step size for discretization is often decided arbitrarily.

	Alternative methods to avoid these shortcomings have been proposed. The simplest of all consists
	on randomly retrieving and evaluating values for the hyperparameters (\emph{random search}), and
	is studied in the context of SML optimization in \cite{bergstra2012random}. This method is
	obviously very slow as it relies only on chance to find good configurations. It does, however,
	allow for a uniform exploration of the hyperparameter space.

	A method based on hierarchical density estimation, and another method based on hierarchical
	Gaussian processes, are proposed in \cite{bergstra2011hyperparameter}. These methods are limited
	to the optimization of hyperparameter values from a single SML algorithm, and do not consider a
	systematic selection of a model. Furthermore, the work lacks a statistical analysis of the
	optimized hyperparameters, and disregards the generalization of the selected configuration to
	unseen data.

	The approach implemented in Auto-WEKA (\cite{autoweka}) does consider model selection and
	hyperparameter optimization simultaneosly. Their work models the hyperparameter space in a
	hierarchical way. Auto-WEKA considers generalization as the only criterion for both optimization and
	model selection.
	
	Statistical frameworks for model selection on SML are described in \cite{pizarro2002mct} and
	\cite{demsar2006mct}, and some of their ideas have been implemented in the present work.
	%Brief literature review. Bergstra's work \cite{bergstra2011hyperparameter} lacks the statistical
	%analysis, and package is cumbersome to use for a non-expert user
