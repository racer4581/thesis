\chapter{Problem description}

	This work aims to propose a solution for a two-fold problem: find the SML algorithm that works
	best for a given set of data, and find which particular choice of configuration settings for
	that algorithm, if required, yields the best results.
	
	Although both sides of the problem correspond conceptually to different ideas (general approach
	the former, fine-tunning the latter), they are intrinsically linked and should not be treated as
	separate problems. This is especially true for the present work, since a poor choice of
	settings for an otherwise well-suited SML algorithm may be easily outperformed by another, less
	appropriated SML method, applied on the same data.

	The formal description of the problem presented here is given in terms of the classification
	problem. This description extends naturally to the regression problem, but the specific
	terminology for regression has been excluded for the sake of brevity.

	\section{Formal definition}

	The following terms and notation will be used for the rest of this document:

	\begin{itemize}
		\item
		An {\bf instance} is a representation of an object, encoded as a pair $(\feature, \class)$,
		with $\feature$ a vector of values for attributes (also called \emph{features}) related to
		the object, and $\class$ a mapping of the object to its corresponding class.
		
		\item
		A {\bf dataset} $\alldata = \{(\feature_1, \class_1), (\feature_2, \class_2), \ldots,
		(\feature_N, \class_N)\}$ is a set of instances on which SML is to be applied.
		%$\alldata$ is split into a {\bf selection~set} $\selectionset$ and a {\bf testing~set}
		%$\testset$, such that
		%\begin{equation}
		%	\selectionset \cup \testset = \alldata \\
		%	\selectionset \cap \testset = \emptyset \\
		%	\selectionset \neq \emptyset
		%\end{equation}

		\item
		An {\bf algorithm} $\algo$ is a specific implementation of any function that uses known
		instances~$(\feature, \class)$ (also known as \emph{examples}) to learn patterns or rules to
		associate the feature values $\feature$ to their class $\class$, and uses this
		information to predict the class $\class'$ of unseen instances from their feature values
		$\feature'$.
		\begin{equation}
			A \colon (\{(\feature, \class)\}, \{\feature'\}) \mapsto \{\class'\}
		\end{equation}

		\item
		$\algoset = \{\algo^1, \ldots, \algo^k\}$ represents the set of all algorithms to evaluate.
		
		\item
		$\param^i = (\paramval^i_a, \paramval^i_b, \paramval^i_c,\ldots)$ denotes a set of
		hyperparameter values (or {\bf configuration}) for the $i$-th algorithm in $\algoset$.
		
		\item
		$\paramspace^i$ represents the set of all possible values that $\param^i$ can take (i.e. the
		{\bf hyperparameter~space} of the $i$-th algorithm).

		\item
		An algorithm $\algo$ under a specific configuration $\param$ is called a {\bf model}, and
		represented as $\algo_{\param}$.

		\item
		The {\bf scoring} function $\Score$ measures how good a model $\algo_{\param}$ is at
		predicting the class of unseen instances drawn from the dataset
		$\alldata$.
		\begin{equation}
			\Score \colon \algo^i \in \algoset, \param^i \in \paramspace^i, \alldata \mapsto
			\mathbbm{R}
		\end{equation}
		The scoring function should be designed in such a way that the better the agreement between
		class prediction and true class assignment is, the higher the score.
	\end{itemize}

	It is assumed that the feature values of the examples $\feature$ used for training an algorithm
	and the feature values to predict labeling $\feature'$ are drawn from the same underlying
	distribution, and that instances with similar feature values tend to belong to the same class.

	Under the described context, the model selection and hyperparameter optimization problem can be
	defined as:
	\begin{equation}
		\label{eq:problem}
		\algo^*_{\param^*} = \argmax_{\algo^i \in \algoset, \param^i \in \paramspace^i}
		\Score(\algo^i_{\param^i}, \alldata)
	\end{equation}

	The above equation can be simply stated as "find the algorithm and its parameter values that
	obtain the best score at predicting labels on the given dataset". It is worth noticing that
	equation~\ref{eq:problem} is the general form of the optimization process, and as such, only
	defines the structure and general behavior of the different components of the optimization. In
	practice, additional details such as the implementation of the scoring function, and the actual
	exploration of the (possibly infinite) hyperparameter spaces, must be considered.

	Furthermore, the assumption that a single model $\algo^*_{\param^*}$ will be \emph{significantly} better than the
	rest is not guaranteed, and hence returning multiple models as the result of the
	optimization process should also be considered under certain circumstances.

	\section{Related work}

	A large number of model selection methods exist, most of which rely on statistical hypothesis
	testing, where usually the \emph{model} refers to a probability distribution that describes a
	set of data.

	Autoweka
	
	Surprisingly, despite its importance, few initiatives to address the problem in the context of
	machine learning have emerged.

	Brief literature review. Bergstra's work \cite{bergstra2011hyperparameter} lacks the statistical
	analysis, and package is cumbersome to use for a non-expert user
